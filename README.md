<h1 align="center">$\bf{\large{\color{#6580DD} LoRA-Fine Tuning-for-Stable Diffusion}}$</h1>
<h1 align="center">$\bf{\large{\color{#6580DD} (CaptionAware-Face Style-Adaptation)}}$</h1>

<!-- # LoRA Fine-tuning for Stable Diffusion (Caption-aware Face Style Adaptation) -->

### Caption-aware LoRA Fine-tuning Project

> Few-Shot í™˜ê²½ì—ì„œ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„± í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ Cross-Attention LoRA ì‹¤í—˜ í”„ë¡œì íŠ¸

---

## Overview

ì´ í”„ë¡œì íŠ¸ëŠ” **ì†ŒëŸ‰ ì´ë¯¸ì§€(100ì¥)ë§Œìœ¼ë¡œ Stable Diffusionì˜ ì–¼êµ´ ìƒì„± í’ˆì§ˆì„ ê°œì„ í•˜ê¸° ìœ„í•œ LoRA Fine-tuning ì‹¤í—˜**ì…ë‹ˆë‹¤.  
íŠ¹íˆ **Cross-Attention(Wq, Wv)** ë ˆì´ì–´ì— LoRAë¥¼ ì ìš©í•˜ì—¬ **Prompt ë°˜ì˜ë¥ (semantic alignment)** ê³¼ **identity consistency**ë¥¼ ë†’ì´ëŠ” ê²ƒì„ ëª©í‘œë¡œ í–ˆìŠµë‹ˆë‹¤.

ë³¸ ì‹¤í—˜ì€ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ì˜ **ë°ì´í„° íš¨ìœ¨í™”(PEFT)** ì „ëµì„ êµ¬ì¡°ì ìœ¼ë¡œ ê²€ì¦í•˜ê¸° ìœ„í•œ ê²ƒì…ë‹ˆë‹¤.

### Key Features

- **LoRA(PEFT)** ê¸°ë°˜ ì €ë­í¬ íŒŒì¸íŠœë‹
- **Cross-Attention(Wq/Wv)** ëŒ€ìƒ ì ìš© ë° rank/Î± ì¡°ì ˆ ì‹¤í—˜
- **BLIP caption + ìˆ˜ë™ ê²€ìˆ˜** ê¸°ë°˜ Caption-aware dataset êµ¬ì„±
- **CLIPScore ê¸°ë°˜ ìë™ í‰ê°€ íŒŒì´í”„ë¼ì¸ êµ¬í˜„**
- **Baseline vs LoRA í’ˆì§ˆ ë¹„êµ**

---

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```plaintext
.
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ images/                 # 100 training images
â”‚   â”œâ”€â”€ captions_example.json
â”‚   â”œâ”€â”€ metadata_example.csv    # BLIP + manual revised captions
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ lora/
â”‚   â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train_lora.py           # LoRA fine-tuning script
â”‚   â”œâ”€â”€ inference.py
â”‚   â””â”€â”€ evaluate_clip.py
â”‚
â””â”€â”€ results/
    â”œâ”€â”€ baseline_01.png
    â”œâ”€â”€ lora_01.png
    â”œâ”€â”€ clip_comparison.png
    â””â”€â”€ examples.png

â””â”€â”€ notebooks/clipscore_...ipynbëŠ” ì‹¤í—˜ ë¡œê·¸ ì°¸ê³ ìš©
```

### Dataset & Captioning

- ì´ 100ì¥ì˜ í•œêµ­ ì—¬ì ì—°ì˜ˆì¸ ì–¼êµ´ ì´ë¯¸ì§€ ì‚¬ìš©
- ì¡°ëª…, ê°ë„, í•´ìƒë„ ë“±ì„ ê¸°ì¤€ìœ¼ë¡œ ìˆ˜ë™ í•„í„°ë§ í›„ í•™ìŠµìš©ìœ¼ë¡œ ì„ ì •
- BLIPìœ¼ë¡œ 1ì°¨ caption ìƒì„± í›„, ë…¸ì´ì¦ˆê°€ ì‹¬í•œ ì¼ë¶€ ìƒ˜í”Œì€ ìˆ˜ë™ìœ¼ë¡œ ìˆ˜ì •
- ìº¡ì…˜ì€ ì–¼êµ´ identityë¥¼ ì§ì ‘ì ìœ¼ë¡œ ë“œëŸ¬ë‚´ê¸°ë³´ë‹¤, í—¤ì–´/í‘œì •/ì¡°ëª… ë“± ìŠ¤íƒ€ì¼ ìœ„ì£¼ë¡œ ê¸°ìˆ 

> ë ˆí¬ì—ëŠ” ì €ì‘ê¶Œ ì´ìŠˆë¥¼ í”¼í•˜ê¸° ìœ„í•´ `datasets/captions_example.json`, `metadata_example.csv`ë¡œ
> **ë°ì´í„° êµ¬ì¡° ìƒ˜í”Œë§Œ** í¬í•¨í•©ë‹ˆë‹¤.

### Training Details

- `accelerate` ê¸°ë°˜ mixed-precision(FP16) í•™ìŠµ ë° gradient accumulation ì ìš©
- `CaptionedDataset`ìœ¼ë¡œ ì´ë¯¸ì§€/ìº¡ì…˜ì„ `metadata.csv`, `captions.json`ì—ì„œ ì§ì ‘ ë¡œë”©
- UNetì˜ Cross-Attention ëª¨ë“ˆ(`attn1`, `attn2`)ì—ë§Œ `LoRAAttnProcessor` ì ìš©
- í•™ìŠµ ì¢…ë£Œ í›„ LoRA íŒŒë¼ë¯¸í„°ë§Œ í•„í„°ë§í•´ ë³„ë„ ë””ë ‰í„°ë¦¬ì— ì €ì¥í•˜ì—¬,
  base ëª¨ë¸ê³¼ ë…ë¦½ì ìœ¼ë¡œ ë¡œë“œ/ì ìš© ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„

## â–¶ How to Run

### 1) LoRA Training

```bash
# Colab ì‚¬ìš©
# ê²½ë¡œ ìˆ˜ì •í•´ì„œ ì‚¬ìš©
!accelerate launch content/train_lora.py \
  --pretrained_model_name_or_path=runwayml/stable-diffusion-v1-5 \
  --instance_data_dir=content/idol_faces_clean \
  --output_dir=content/lora_outputs \
  --caption_file=captions.json \
  --resolution=384 \
  --train_batch_size=1 \
  --num_train_epochs=10 \
  --learning_rate=5e-5 \
  --mixed_precision="fp16" \
  --gradient_accumulation_steps=2 \
  --lr_scheduler="constant" \
  --checkpointing_steps=100 \
  --rank=4 \
  --alpha=8


python src/inference.py \
  --pretrained_model runwayml/stable-diffusion-v1-5 \
  --lora_weights ./lora_weights \
  --prompt "korean actress, natural daylight portrait"


python src/evaluate_clip.py --images_dir ./results

```

## ì‹¤í—˜ íŒŒì´í”„ë¼ì¸

<!-- ![Pipeline](notebooks/images/default_diagram.png) -->

<img src="notebooks/images/default_diagram.png" width="50%" height="auto" alt="Pipeline">

## LoRA in Cross-Attention

ì•„ë˜ ê·¸ë¦¼ì€ Stable Diffusion UNetì˜ Cross-Attention ë¸”ë¡ì—ì„œ
Query/Value projectionì—ë§Œ LoRAë¥¼ ì ìš©í•œ êµ¬ì¡°ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

![LoRA in Cross-Attention](notebooks/images/UNet_Attention_Mechanism_Diagram.jpeg)

- Text encoderì—ì„œ ë‚˜ì˜¨ embeddingì´ K, Vë¡œ ë“¤ì–´ê°€ê³ ,
- UNet latentê°€ Që¡œ íˆ¬ì…ë©ë‹ˆë‹¤.
- ì´ë•Œ Q, V projection layerì—ë§Œ Î”W = BA í˜•íƒœì˜ LoRAë¥¼ ì ìš©í•´
  í…ìŠ¤íŠ¸ ì¡°ê±´ ë°˜ì˜ ê²½ë¡œë§Œ ì„ íƒì ìœ¼ë¡œ ë³´ì •í•©ë‹ˆë‹¤.

## ğŸ“ˆPerformance (CLIPScore)

| Model           | CLIPScore        |
| --------------- | ---------------- |
| Baseline        | 0.51             |
| LoRA Fine-tuned | **0.55 (+7.8%)** |

**â†’ caption-aware fine-tuningì´ identity ìœ ì§€ ë° ìŠ¤íƒ€ì¼ ë°˜ì˜ì— íš¨ê³¼ì ì„ì„ í™•ì¸**

---
